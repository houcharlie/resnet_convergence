{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "relaxations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0pVQC4ZrsoBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAgVZX10WdfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "W2(Relu((I+W)x))"
      ]
    },
    {
      "metadata": {
        "id": "jWwEL0bctulD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, d):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(d, d, bias = False)\n",
        "        self.fc2 = nn.Linear(d, d, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y = self.fc2(F.relu(self.fc1(x) + x))\n",
        "        return torch.sum(y)\n",
        "\n",
        "class TeacherNet(object):\n",
        "  def __init__(self, d):\n",
        "    self.W1 = torch.rand(d,d)\n",
        "    self.W2 = torch.rand(d,d)\n",
        "  def compute(self, x):\n",
        "    return torch.sum(torch.mv(self.W2,F.relu(torch.mv(self.W1, x) + x)))\n",
        "  def getWeights(self):\n",
        "    return (self.W1, self.W2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xadtC-_xWXTJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "W2(Relu((I+W)x)) + x"
      ]
    },
    {
      "metadata": {
        "id": "1QQZB2dM2Nxn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, d):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(d, d, bias = False)\n",
        "        self.fc2 = nn.Linear(d, d, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y = self.fc2(F.relu(self.fc1(x) + x)) + x\n",
        "        return torch.sum(y)\n",
        "\n",
        "class TeacherNet(object):\n",
        "  def __init__(self, d):\n",
        "    self.W1 = torch.rand(d,d)\n",
        "    self.W2 = torch.rand(d,d)\n",
        "  def compute(self, x):\n",
        "    return torch.sum(torch.mv(self.W2,F.relu(torch.mv(self.W1, x) + x)) + x)\n",
        "  def getWeights(self):\n",
        "    return (self.W1, self.W2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KHnyM4alvRRc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = 10\n",
        "\n",
        "teacher = TeacherNet(d)\n",
        "inputs = []\n",
        "trueOutputs = []\n",
        "for i in range(100000):\n",
        "  # x is sampled N 0,1\n",
        "  x = np.random.multivariate_normal(np.zeros(d), np.identity(d))\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.float()\n",
        "  y = teacher.compute(x)\n",
        "  inputs.append(x)\n",
        "  trueOutputs.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVQMF7ctvVxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, featureList, outputList):\n",
        "    self.featureList = featureList\n",
        "    self.outputList = outputList\n",
        "    assert len(featureList) == len(outputList)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.featureList[idx], self.outputList[idx]\n",
        "  def __len__(self):\n",
        "    return len(self.featureList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uPfI3B_WvfGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data as utils\n",
        "dataset = MyDataset(inputs, trueOutputs)\n",
        "trainloader = utils.DataLoader(dataset, batch_size = 200,\n",
        "                               shuffle = True, num_workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yIMEPVTAvk3u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "net = Net(d)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum = .01, nesterov = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ObJQZRzvn9G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2017
        },
        "outputId": "11b8cc62-6950-4bb4-ab96-10bebe482822"
      },
      "cell_type": "code",
      "source": [
        "iterations = 0\n",
        "losses = []\n",
        "iterationNumbers = []\n",
        "num_batches = 250\n",
        "epochs = 100\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      # get the inputs\n",
        "      features, values = data\n",
        "      \n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      outputlist = []\n",
        "      for feature in features:\n",
        "        outputlist.append(net(feature))\n",
        "      outputs = torch.stack([j for j in outputlist])\n",
        "      loss = criterion(outputs, values)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      \n",
        "      if i % num_batches == (num_batches - 1):\n",
        "        # find the distance between parameters\n",
        "        params = list(net.parameters())\n",
        "        studentW1 = params[0]\n",
        "        studentW2 = params[1]\n",
        "        (teacherW1, teacherW2) = teacher.getWeights()\n",
        "        distance = np.linalg.norm(studentW1.data.numpy() - teacherW1.data.numpy()) + np.linalg.norm(studentW2.data.numpy() - teacherW2.data.numpy())\n",
        "        studentW1Norm = np.linalg.norm(studentW1.data.numpy())\n",
        "        studentW2Norm = np.linalg.norm(studentW2.data.numpy())\n",
        "        losses.append(running_loss/num_batches)\n",
        "        iterationNumbers.append(iterations)\n",
        "        print('[%d, %5d] loss: %.3f, distance: %.4f, norm1: %.3f, norm2: %.3f' %\n",
        "            (epoch + 1, i + 1, running_loss/num_batches, distance, \n",
        "            studentW1Norm, studentW2Norm))\n",
        "        running_loss = 0.0\n",
        "      iterations += 1\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   250] loss: 130.085, distance: 8.3565, norm1: 4.950, norm2: 5.959\n",
            "[1,   500] loss: 6.591, distance: 7.7387, norm1: 4.945, norm2: 5.956\n",
            "[2,   250] loss: 4.124, distance: 7.3087, norm1: 4.939, norm2: 5.948\n",
            "[2,   500] loss: 2.908, distance: 7.0059, norm1: 4.955, norm2: 5.952\n",
            "[3,   250] loss: 2.130, distance: 6.7899, norm1: 4.958, norm2: 5.946\n",
            "[3,   500] loss: 1.663, distance: 6.6234, norm1: 4.966, norm2: 5.949\n",
            "[4,   250] loss: 1.359, distance: 6.4973, norm1: 4.971, norm2: 5.948\n",
            "[4,   500] loss: 1.151, distance: 6.3945, norm1: 4.971, norm2: 5.945\n",
            "[5,   250] loss: 0.994, distance: 6.3215, norm1: 4.977, norm2: 5.946\n",
            "[5,   500] loss: 0.861, distance: 6.2623, norm1: 4.980, norm2: 5.947\n",
            "[6,   250] loss: 0.783, distance: 6.2186, norm1: 4.983, norm2: 5.946\n",
            "[6,   500] loss: 0.715, distance: 6.1833, norm1: 4.985, norm2: 5.947\n",
            "[7,   250] loss: 0.674, distance: 6.1628, norm1: 4.989, norm2: 5.947\n",
            "[7,   500] loss: 0.630, distance: 6.1453, norm1: 4.992, norm2: 5.946\n",
            "[8,   250] loss: 0.613, distance: 6.1362, norm1: 4.996, norm2: 5.945\n",
            "[8,   500] loss: 0.577, distance: 6.1352, norm1: 5.003, norm2: 5.945\n",
            "[9,   250] loss: 0.562, distance: 6.1372, norm1: 5.011, norm2: 5.947\n",
            "[9,   500] loss: 0.539, distance: 6.1401, norm1: 5.018, norm2: 5.946\n",
            "[10,   250] loss: 0.510, distance: 6.1448, norm1: 5.026, norm2: 5.945\n",
            "[10,   500] loss: 0.489, distance: 6.1483, norm1: 5.036, norm2: 5.945\n",
            "[11,   250] loss: 0.449, distance: 6.1477, norm1: 5.046, norm2: 5.945\n",
            "[11,   500] loss: 0.424, distance: 6.1452, norm1: 5.053, norm2: 5.945\n",
            "[12,   250] loss: 0.388, distance: 6.1325, norm1: 5.061, norm2: 5.944\n",
            "[12,   500] loss: 0.353, distance: 6.1169, norm1: 5.066, norm2: 5.944\n",
            "[13,   250] loss: 0.322, distance: 6.0954, norm1: 5.069, norm2: 5.943\n",
            "[13,   500] loss: 0.305, distance: 6.0694, norm1: 5.071, norm2: 5.943\n",
            "[14,   250] loss: 0.282, distance: 6.0445, norm1: 5.073, norm2: 5.942\n",
            "[14,   500] loss: 0.251, distance: 6.0144, norm1: 5.073, norm2: 5.941\n",
            "[15,   250] loss: 0.235, distance: 5.9878, norm1: 5.074, norm2: 5.943\n",
            "[15,   500] loss: 0.214, distance: 5.9640, norm1: 5.075, norm2: 5.941\n",
            "[16,   250] loss: 0.197, distance: 5.9399, norm1: 5.075, norm2: 5.941\n",
            "[16,   500] loss: 0.175, distance: 5.9186, norm1: 5.076, norm2: 5.941\n",
            "[17,   250] loss: 0.157, distance: 5.8964, norm1: 5.075, norm2: 5.941\n",
            "[17,   500] loss: 0.144, distance: 5.8801, norm1: 5.077, norm2: 5.941\n",
            "[18,   250] loss: 0.125, distance: 5.8650, norm1: 5.077, norm2: 5.940\n",
            "[18,   500] loss: 0.112, distance: 5.8479, norm1: 5.079, norm2: 5.940\n",
            "[19,   250] loss: 0.098, distance: 5.8359, norm1: 5.079, norm2: 5.940\n",
            "[19,   500] loss: 0.084, distance: 5.8245, norm1: 5.080, norm2: 5.941\n",
            "[20,   250] loss: 0.074, distance: 5.8152, norm1: 5.080, norm2: 5.940\n",
            "[20,   500] loss: 0.064, distance: 5.8073, norm1: 5.082, norm2: 5.940\n",
            "[21,   250] loss: 0.055, distance: 5.7996, norm1: 5.083, norm2: 5.940\n",
            "[21,   500] loss: 0.047, distance: 5.7936, norm1: 5.083, norm2: 5.940\n",
            "[22,   250] loss: 0.040, distance: 5.7886, norm1: 5.084, norm2: 5.940\n",
            "[22,   500] loss: 0.033, distance: 5.7838, norm1: 5.085, norm2: 5.940\n",
            "[23,   250] loss: 0.028, distance: 5.7805, norm1: 5.086, norm2: 5.940\n",
            "[23,   500] loss: 0.024, distance: 5.7782, norm1: 5.087, norm2: 5.940\n",
            "[24,   250] loss: 0.020, distance: 5.7753, norm1: 5.088, norm2: 5.940\n",
            "[24,   500] loss: 0.017, distance: 5.7738, norm1: 5.089, norm2: 5.940\n",
            "[25,   250] loss: 0.014, distance: 5.7718, norm1: 5.089, norm2: 5.940\n",
            "[25,   500] loss: 0.012, distance: 5.7704, norm1: 5.089, norm2: 5.940\n",
            "[26,   250] loss: 0.010, distance: 5.7690, norm1: 5.090, norm2: 5.940\n",
            "[26,   500] loss: 0.008, distance: 5.7681, norm1: 5.090, norm2: 5.940\n",
            "[27,   250] loss: 0.007, distance: 5.7677, norm1: 5.091, norm2: 5.940\n",
            "[27,   500] loss: 0.005, distance: 5.7670, norm1: 5.091, norm2: 5.940\n",
            "[28,   250] loss: 0.005, distance: 5.7668, norm1: 5.092, norm2: 5.940\n",
            "[28,   500] loss: 0.004, distance: 5.7662, norm1: 5.092, norm2: 5.940\n",
            "[29,   250] loss: 0.003, distance: 5.7658, norm1: 5.093, norm2: 5.940\n",
            "[29,   500] loss: 0.003, distance: 5.7656, norm1: 5.093, norm2: 5.940\n",
            "[30,   250] loss: 0.002, distance: 5.7655, norm1: 5.093, norm2: 5.940\n",
            "[30,   500] loss: 0.002, distance: 5.7653, norm1: 5.093, norm2: 5.940\n",
            "[31,   250] loss: 0.001, distance: 5.7651, norm1: 5.094, norm2: 5.940\n",
            "[31,   500] loss: 0.001, distance: 5.7651, norm1: 5.094, norm2: 5.940\n",
            "[32,   250] loss: 0.001, distance: 5.7651, norm1: 5.094, norm2: 5.940\n",
            "[32,   500] loss: 0.001, distance: 5.7650, norm1: 5.094, norm2: 5.940\n",
            "[33,   250] loss: 0.001, distance: 5.7648, norm1: 5.094, norm2: 5.940\n",
            "[33,   500] loss: 0.001, distance: 5.7650, norm1: 5.095, norm2: 5.940\n",
            "[34,   250] loss: 0.000, distance: 5.7650, norm1: 5.095, norm2: 5.940\n",
            "[34,   500] loss: 0.000, distance: 5.7649, norm1: 5.095, norm2: 5.940\n",
            "[35,   250] loss: 0.000, distance: 5.7649, norm1: 5.095, norm2: 5.940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-1323:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
            "Process Process-1324:\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
            "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
            "    self.run()\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
            "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 131, in get\n",
            "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 131, in get\n",
            "KeyboardInterrupt\n",
            "    if not self._poll(timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-1e5e2ace7b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "10itSp3UvvyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "2abb7d23-dc1b-451d-afd3-2b774dc8a0dc"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "plt.plot(iterationNumbers, losses)\n",
        "plt.xlabel(\"Batch number\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFYCAYAAACoFn5YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f572a721510>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "egWVcsgLWOGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}